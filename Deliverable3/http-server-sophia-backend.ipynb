{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def load_model(model_name, vector_name):\n",
    "    model = pickle.load(open(model_name, 'rb'))\n",
    "    vectorizer = pickle.load(open(vector_name, 'rb'))\n",
    "    return model, vectorizer\n",
    "\n",
    "def predict(sentence, model, vectorizer):\n",
    "    vector = vectorizer.transform([sentence])\n",
    "    prediction = model.predict(vector)[0]\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentence(device, model, initial_seed, vocab_to_int, int_to_vocab, top_k=5, length=50):\n",
    "    def get_word_index(w):\n",
    "        if w not in vocab_to_int: \n",
    "            return 0\n",
    "        return vocab_to_int[w]\n",
    "\n",
    "    model.eval()\n",
    "    # words = ['I', 'am']\n",
    "    words = initial_seed.split()\n",
    "\n",
    "    state_h, state_c = model.zero_state(1)\n",
    "    state_h = state_h.to(device)\n",
    "    state_c = state_c.to(device)\n",
    "    for w in words:\n",
    "        ix = torch.tensor([[get_word_index(w)]]).to(device)\n",
    "        # print(ix)\n",
    "        output, (state_h, state_c) = model(ix, (state_h, state_c))\n",
    "\n",
    "    _, top_ix = torch.topk(output[0], k=top_k)\n",
    "    choices = top_ix.tolist()\n",
    "    choice = np.random.choice(choices[0])\n",
    "\n",
    "    words.append(int_to_vocab[choice])\n",
    "    \n",
    "#     for _ in range(length):\n",
    "    while (len(words) > length and words[len(words)-1].strip().endswith('.')) == False:\n",
    "        ix = torch.tensor([[choice]]).to(device)\n",
    "        output, (state_h, state_c) = model(ix, (state_h, state_c))\n",
    "\n",
    "        _, top_ix = torch.topk(output[0], k=top_k)\n",
    "        choices = top_ix.tolist()\n",
    "        choice = np.random.choice(choices[0])\n",
    "        words.append(int_to_vocab[choice])\n",
    "\n",
    "    return (' '.join(words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class AutoRegressive(nn.Module):\n",
    "    def __init__(self, vocabulary_length, sequence_size, embedding_size, lstm_size):\n",
    "        super(AutoRegressive, self).__init__()\n",
    "        self.seq_size = sequence_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocabulary_length, \n",
    "            embedding_size\n",
    "            )\n",
    "        self.lstm = nn.LSTM(embedding_size,\n",
    "                            lstm_size,\n",
    "                            batch_first=False)\n",
    "        self.dense = nn.Linear(lstm_size, vocabulary_length)\n",
    "    \n",
    "    def forward(self, input, previous_state ): \n",
    "        embed = self.embedding(input)\n",
    "        output, state = self.lstm(embed, previous_state)\n",
    "        logits = self.dense(output)\n",
    "\n",
    "        return logits, state\n",
    "\n",
    "    def zero_state(self, batch_size):\n",
    "        return (torch.zeros(1, batch_size, self.lstm_size),\n",
    "                torch.zeros(1, batch_size, self.lstm_size))\n",
    "\n",
    "\n",
    "def _load_neural_network(model_name):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    checkpoint = torch.load(model_name, map_location=device)\n",
    "    words = checkpoint['words']\n",
    "    \n",
    "    model = AutoRegressive(len(words), 32, 64, 16)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    int_to_vocab = checkpoint['int_to_vocab']\n",
    "    vocab_to_int = checkpoint['vocab_to_int']\n",
    "    \n",
    "    return device, model, words, int_to_vocab, vocab_to_int\n",
    "\n",
    "def load_story_model():\n",
    "    file_name = 'story/story.pt'\n",
    "    device, model, words, int_to_vocab, vocab_to_int = _load_neural_network(file_name)\n",
    "    \n",
    "    def run_prediction(initial_words, min_length=50):\n",
    "        return predict_sentence(device, model, initial_words, vocab_to_int, int_to_vocab, length=min_length)\n",
    "    \n",
    "    return run_prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict = load_story_model()\n",
    "# predict('queen grabbed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_model_predict(model, vector):\n",
    "    m, v = load_model(model, vector)\n",
    "    \n",
    "    return lambda sentence: predict(sentence, m, v)\n",
    "    \n",
    "def load_model1():\n",
    "    model = 'model1/model.pickle'\n",
    "    vector = 'model1/vector.pickle'\n",
    "    return _load_model_predict(model, vector)\n",
    "\n",
    "def load_model2():\n",
    "    model = 'model2/model.pickle'\n",
    "    vector = 'model2/vector.pickle'\n",
    "    return _load_model_predict(model, vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/Users/msarica/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/Users/msarica/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '/Users/msarica/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': '/Users/msarica/stanfordnlp_resources/en_ewt_models/en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "---\n",
      "Loading: depparse\n",
      "With settings: \n",
      "{'model_path': '/Users/msarica/stanfordnlp_resources/en_ewt_models/en_ewt_parser.pt', 'pretrain_path': '/Users/msarica/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# https://stanfordnlp.github.io/stanfordnlp/\n",
    "# !pip install stanfordnlp\n",
    "\n",
    "import stanfordnlp\n",
    "# stanfordnlp.download('en')   # This downloads the English models for the neural pipeline\n",
    "nlp = stanfordnlp.Pipeline() # This sets up a default neural pipeline in English\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def generate_time_units():\n",
    "    time_units = [\n",
    "        ('second', 1),\n",
    "        ('minute', 60),\n",
    "        ('hour', 3600),\n",
    "        ('day', 3600 * 24)\n",
    "    ];\n",
    "\n",
    "    times = []\n",
    "    for word, unit in time_units:\n",
    "        s = '\\w+(?=\\s'+ word +')'\n",
    "        r = re.compile(s, re.IGNORECASE)\n",
    "        times.append((r, unit))\n",
    "    return times\n",
    "\n",
    "times = generate_time_units()\n",
    "\n",
    "def timer(sentence):\n",
    "    def parse_int(value):\n",
    "        try:\n",
    "            return int(value.strip())\n",
    "        except: \n",
    "            return 0\n",
    "        \n",
    "    def convert_to_sec(regex_result, value_to_sec):\n",
    "        if regex_result == None:\n",
    "            return 0\n",
    "        \n",
    "        value = parse_int(regex_result[0])\n",
    "        return value * value_to_sec\n",
    "    \n",
    "    total = 0\n",
    "    for r, unit in times:\n",
    "        m = r.search(sentence)\n",
    "        total += convert_to_sec(m, unit)\n",
    "        \n",
    "    if total == 0:\n",
    "        m = re.search(r'\\d+', sentence)\n",
    "        total += convert_to_sec(m, 1)\n",
    "        \n",
    "    return total\n",
    "\n",
    "# timer('set a timer for 3 minutes 30 seconds')\n",
    "# timer('sadfa 5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_object(sentence, nlp=nlp):\n",
    "    doc = nlp(sentence)\n",
    "    # doc.sentences[0].print_dependencies()\n",
    "\n",
    "    objs = []\n",
    "    for i in doc.sentences[0].words:\n",
    "        # print(i.dependency_relation)\n",
    "        if(i.dependency_relation == 'obj'):\n",
    "            objs.append(i.text)\n",
    "    \n",
    "    return objs\n",
    "    \n",
    "\n",
    "# get_object('sophia set a timer for 5 minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tell_story(sentence):\n",
    "    m = re.search(r'(?<=about\\s)[\\w\\s]*', sentence)\n",
    "    \n",
    "    initial = m[0]\n",
    "    return story_model(initial)\n",
    "    \n",
    "# tell_story('tell me a story about snow white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def time_now():\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%I:%M %p\")\n",
    "    return f\"it is {current_time}\"\n",
    "\n",
    "# time_now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = load_model1()\n",
    "predict_category = load_model2()\n",
    "\n",
    "story_model = load_story_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(sentence):\n",
    "    # predict category\n",
    "    category = predict_category(sentence)\n",
    "    print(category)\n",
    "    \n",
    "    actions = {\n",
    "        'timer': lambda sent: timer(sent),\n",
    "        'shopping': lambda sent: get_object(sent),\n",
    "        'time_now': lambda sent: time_now(),\n",
    "        'story': lambda sent: tell_story(sent)\n",
    "    }\n",
    "    \n",
    "    if category in actions:\n",
    "        text = actions[category](sentence)\n",
    "    else:\n",
    "        text = None\n",
    "    \n",
    "    return category, text\n",
    "    \n",
    "# process('sophia can you add milk to my list')\n",
    "# process('set a timer for 5 ')\n",
    "# process('what time is it')\n",
    "# process('sophia please tell me a story about a witch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [03/Aug/2020 22:37:23] \"\u001b[37mOPTIONS /?sentence=Sophia%20tell%20me%20a%20story%20about%20my%20son HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [03/Aug/2020 22:37:23] \"\u001b[37mGET /?sentence=Sophia%20tell%20me%20a%20story%20about%20my%20son HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sophia tell me a story about my son\n",
      "story\n",
      "story my son like no peace day heart in her the wild forest, the poor proof that hour, queen as white looked more at all the lung in front a token. i a beautiful woman, soon took them lady thou window at all or night. the looking-glass snow, answered, thou, thou, spoke looking-glass, and pride beautiful snow, as red snow, snow, she began that as proof had passed she pricked he no that it seemed and said, ah dear run away, to kill more come looking-glass, snow, she called a child the wild never home queen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [03/Aug/2020 22:38:30] \"\u001b[37mOPTIONS /?sentence=Sophia HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [03/Aug/2020 22:38:30] \"\u001b[37mGET /?sentence=Sophia HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sophia\n",
      "timer\n",
      "timer 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [03/Aug/2020 22:40:11] \"\u001b[37mOPTIONS /?sentence=Sofia%20set%20timer%20for%203%20minutes HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [03/Aug/2020 22:40:11] \"\u001b[37mGET /?sentence=Sofia%20set%20timer%20for%203%20minutes HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sofia set timer for 3 minutes\n",
      "timer\n",
      "timer 180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [03/Aug/2020 22:41:00] \"\u001b[37mOPTIONS /?sentence=Sofia%20add%20bread%20to%20my%20shopping%20list HTTP/1.1\u001b[0m\" 200 -\n",
      "/Users/distiller/project/conda/conda-bld/pytorch_1591914925853/work/aten/src/ATen/native/BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n",
      "/Users/distiller/project/conda/conda-bld/pytorch_1591914925853/work/aten/src/ATen/native/TensorAdvancedIndexing.cpp:573: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "127.0.0.1 - - [03/Aug/2020 22:41:00] \"\u001b[37mGET /?sentence=Sofia%20add%20bread%20to%20my%20shopping%20list HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sofia add bread to my shopping list\n",
      "shopping\n",
      "shopping ['bread']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [03/Aug/2020 22:44:28] \"\u001b[37mOPTIONS /?sentence=Sophia HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [03/Aug/2020 22:44:28] \"\u001b[37mGET /?sentence=Sophia HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sophia\n",
      "timer\n",
      "timer 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [03/Aug/2020 22:44:34] \"\u001b[37mOPTIONS /?sentence=Sofia%20set%20a%20timer%20for%205%20minutes HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [03/Aug/2020 22:44:34] \"\u001b[37mGET /?sentence=Sofia%20set%20a%20timer%20for%205%20minutes HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sofia set a timer for 5 minutes\n",
      "timer\n",
      "timer 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [03/Aug/2020 22:44:46] \"\u001b[37mOPTIONS /?sentence=Sofia%20add%20bread%20to%20my%20shopping%20list HTTP/1.1\u001b[0m\" 200 -\n",
      "/Users/distiller/project/conda/conda-bld/pytorch_1591914925853/work/aten/src/ATen/native/TensorAdvancedIndexing.cpp:573: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "127.0.0.1 - - [03/Aug/2020 22:44:46] \"\u001b[37mGET /?sentence=Sofia%20add%20bread%20to%20my%20shopping%20list HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sofia add bread to my shopping list\n",
      "shopping\n",
      "shopping ['bread']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [03/Aug/2020 22:45:03] \"\u001b[37mOPTIONS /?sentence=Sophia%20tell%20a%20story%20about%20my%20beautiful%20wife HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [03/Aug/2020 22:45:03] \"\u001b[37mGET /?sentence=Sophia%20tell%20a%20story%20about%20my%20beautiful%20wife HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sophia tell a story about my beautiful wife\n",
      "story\n",
      "story my beautiful wife back my sight. kill her, envy. in her finger day have run away needful queen. front thou, pretty him looking-glass, to salt them, a token. and said, ah soon into if eaten the lung in her in my life. and three its did not and liver of it and looked at so looking-glass, on the queen looked pretty now thought to salt in in her away forest, no beautiful if a stone had soon after himself fell eaten terrified had looking-glass, looking-glass, looking-glass, of snow-white.\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask\n",
    "from flask import jsonify, request\n",
    "from flask_cors import CORS\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "@app.route('/')\n",
    "def start():\n",
    "    sentence = request.args.get('sentence')\n",
    "    print(sentence)\n",
    "\n",
    "    category, text = process(sentence)\n",
    "    print(category, text)\n",
    "    return jsonify({\n",
    "        'category': category,\n",
    "        'text': text\n",
    "    })\n",
    "if __name__ == '__main__':\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
