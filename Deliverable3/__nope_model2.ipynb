{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision \n",
    "from torchvision import transforms\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class View(nn.Module):\n",
    "    def __init__(self, *shape):\n",
    "        super(View, self).__init__()\n",
    "        self.shape = shape\n",
    "    def forward(self, input):\n",
    "        return input.view(*self.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_reset(m):\n",
    "    \"\"\"\n",
    "    Go through a PyTorch module m and reset all the weights to an initial random state\n",
    "    \"\"\"\n",
    "    if \"reset_parameters\" in dir(m):\n",
    "        m.reset_parameters()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lesson_folder='/Users/msarica/Desktop/DS606/DS_Capstone/Deliverable3/'\n",
    "def delete_file(file_name):\n",
    "    file = lesson_folder + file_name\n",
    "    if os.path.exists(file) == False:\n",
    "        print (\"file doesn't exist\")\n",
    "    else:\n",
    "        os.remove(file)\n",
    "\n",
    "# delete_file('model.pt')\n",
    "\n",
    "def get_filename(file_name):\n",
    "    return  lesson_folder + file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(\n",
    "    model, \n",
    "    loss_func, \n",
    "    train_loader, \n",
    "    val_loader=None, \n",
    "    score_funcs=None,\n",
    "    epochs=50, \n",
    "    device=\"cpu\", \n",
    "    checkpoint_file=None,\n",
    "    lr_schedule=None, \n",
    "    optimizer=None, \n",
    "    disable_tqdm=False,\n",
    "    should_stop_early=None,\n",
    "    override_early_stop_on_start=False\n",
    "):\n",
    "    \"\"\"Train simple neural networks\n",
    "    \n",
    "    Keyword arguments:\n",
    "    model -- the PyTorch model / \"Module\" to train\n",
    "    loss_func -- the loss function that takes in batch in two arguments, the model outputs and the labels, and returns a score\n",
    "    train_loader -- PyTorch DataLoader object that returns tuples of (input, label) pairs. \n",
    "    val_loader -- Optional PyTorch DataLoader to evaluate on after every epoch\n",
    "    score_funcs -- A dictionary of scoring functions to use to evalue the performance of the model\n",
    "    epochs -- the number of training epochs to perform\n",
    "    device -- the compute lodation to perform training\n",
    "    \n",
    "    \"\"\"\n",
    "    if score_funcs == None:\n",
    "        score_funcs = {}#Empty set \n",
    "    \n",
    "    to_track = [\"epoch\", \"total time\", \"train loss\"]\n",
    "    if val_loader is not None:\n",
    "        to_track.append(\"val loss\")\n",
    "    for eval_score in score_funcs:\n",
    "        to_track.append(\"train \" + eval_score )\n",
    "        if val_loader is not None:\n",
    "            to_track.append(\"val \" + eval_score )\n",
    "        \n",
    "    total_train_time = 0 #How long have we spent in the training loop? \n",
    "    results = {}\n",
    "    #Initialize every item with an empty list\n",
    "    for item in to_track:\n",
    "        results[item] = []\n",
    "\n",
    "    if optimizer == None:\n",
    "        #The AdamW optimizer is a good default optimizer\n",
    "        optimizer = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "    #Place the model on the correct compute resource (CPU or GPU)\n",
    "    model.to(device)\n",
    "\n",
    "    last_epoch = None\n",
    "    if checkpoint_file is not None and os.path.exists(checkpoint_file):\n",
    "      checkpoint = torch.load(checkpoint_file, map_location=device)\n",
    "      model.load_state_dict(checkpoint['model_state_dict'])\n",
    "      optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "      results = checkpoint['results']\n",
    "      early_stop = False if 'early_stop' not in checkpoint else checkpoint['early_stop']\n",
    "      last_epoch = checkpoint['epoch']\n",
    "      print (\"last saved epoch is \" , last_epoch+1)\n",
    "\n",
    "      if override_early_stop_on_start == False and early_stop == True:\n",
    "        print ('early stopped')\n",
    "        return pd.DataFrame.from_dict(results)\n",
    "\n",
    "      if last_epoch == epochs-1:\n",
    "        print ('model fully loaded - no further training needed')\n",
    "\n",
    "    for epoch in tqdm(range(epochs), desc=\"Epoch\", disable=disable_tqdm):\n",
    "        if last_epoch is not None and epoch <= last_epoch:\n",
    "          continue\n",
    "\n",
    "    \n",
    "        model = model.train()#Put our model in training mode\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "\n",
    "        start = time.time()\n",
    "        for inputs, labels in tqdm(train_loader, desc=\"Train Batch\", leave=False, disable=disable_tqdm):\n",
    "      \n",
    "            #Move the batch to the device we are using. \n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            batch_size = labels.shape[0]\n",
    "\n",
    "            # PyTorch stores gradients in a mutable data structure. So we need to set it to a clean state before we use it. \n",
    "            #Otherwise, it will have old information from a previous iteration\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_hat = model(inputs) #this just computed f_Θ(x(i))\n",
    "\n",
    "            # Compute loss.\n",
    "            loss = loss_func(y_hat, labels)\n",
    "\n",
    "            loss.backward()# ∇_Θ just got computed by this one call!\n",
    "\n",
    "            #Now we just need to update all the parameters! \n",
    "            optimizer.step()# Θ_{k+1} = Θ_k − η * ∇_Θ ℓ(y_hat, y)\n",
    "\n",
    "            #Now we are just grabbing some information we would like to have\n",
    "            running_loss += loss.item() * batch_size\n",
    "            \n",
    "            #moving labels & predictions back to CPU for computing / storing predictions\n",
    "            labels = labels.detach().cpu().numpy()\n",
    "            y_hat = y_hat.detach().cpu().numpy()\n",
    "            for i in range(batch_size):\n",
    "                y_true.append(labels[i])\n",
    "                y_pred.append(y_hat[i,:])\n",
    "        #end training epoch\n",
    "        end = time.time()\n",
    "        total_train_time += (end-start)\n",
    "        \n",
    "        results[\"epoch\"].append( epoch )\n",
    "        results[\"total time\"].append( total_train_time )\n",
    "        results[\"train loss\"].append( running_loss )\n",
    "        \n",
    "        y_pred = np.asarray(y_pred)\n",
    "        \n",
    "        if y_pred.shape[1] > 1: #We have a classification problem, convert to labels\n",
    "            y_pred = np.argmax(y_pred, axis=1)\n",
    "            \n",
    "        for name, score_func in score_funcs.items():\n",
    "            results[\"train \" + name].append( score_func(y_true, y_pred) )\n",
    "      \n",
    "        if val_loader is None:\n",
    "            pass\n",
    "        else:#Lets find out validation performance as we go!\n",
    "            model = model.eval() #Set the model to \"evaluation\" mode, b/c we don't want to make any updates!\n",
    "\n",
    "            y_true = []\n",
    "            y_pred = []\n",
    "            \n",
    "            val_running_loss = 0.0\n",
    "\n",
    "            for inputs, labels in val_loader:\n",
    "        \n",
    "                #Move the batch to the device we are using. \n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                batch_size = labels.shape[0]\n",
    "        \n",
    "                y_hat = model(inputs)\n",
    "            \n",
    "                loss = loss_func(y_hat, labels)\n",
    "                \n",
    "                #Now we are just grabbing some information we would like to have\n",
    "                val_running_loss += loss.item() * batch_size\n",
    "\n",
    "                #moving labels & predictions back to CPU for computing / storing predictions\n",
    "                labels = labels.detach().cpu().numpy()\n",
    "                y_hat = y_hat.detach().cpu().numpy()\n",
    "                for i in range(batch_size):\n",
    "                    y_true.append(labels[i])\n",
    "                    y_pred.append(y_hat[i,:])\n",
    "                        \n",
    "            results[\"val loss\"].append( running_loss )\n",
    "\n",
    "            y_pred = np.asarray(y_pred)\n",
    "\n",
    "            if y_pred.shape[1] > 1: #We have a classification problem, convert to labels\n",
    "                y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "            for name, score_func in score_funcs.items():\n",
    "                score_text = \"val \" + name\n",
    "                score_result = score_func(y_true, y_pred)\n",
    "                print (score_text, score_result)\n",
    "                results[score_text].append( score_result )\n",
    "        \n",
    "        #In PyTorch, the convention is to update the learning rate after every epoch\n",
    "        if not lr_schedule is None:\n",
    "            if isinstance(lr_schedule, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                lr_schedule.step(val_running_loss)\n",
    "            else:\n",
    "                lr_schedule.step()\n",
    "        \n",
    "        # early stopping (experiment)\n",
    "        early_stop_flag = False\n",
    "        if should_stop_early is not None and should_stop_early(pd.DataFrame.from_dict(results), epoch):\n",
    "            early_stop_flag = True\n",
    "\n",
    "        if checkpoint_file is not None:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'results' : results,\n",
    "                'early_stop': early_stop_flag\n",
    "                }, checkpoint_file)\n",
    "            \n",
    "            if early_stop_flag == True:\n",
    "                  break\n",
    "            \n",
    "    return pd.DataFrame.from_dict(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "\n",
    "all_data = []\n",
    "# resp = urlopen(\"https://cs.stanford.edu/people/karpathy/char-rnn/shakespear.txt\")\n",
    "resp = urlopen(\"https://norvig.com/big.txt\")\n",
    "shakespear_100k = resp.read()[0:3000000]\n",
    "shakespear_100k = shakespear_100k.decode('utf-8').lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size:  67\n",
      "Total Characters: 3000000\n"
     ]
    }
   ],
   "source": [
    "vocab2indx = {}\n",
    "for char in shakespear_100k:\n",
    "    if char not in vocab2indx:\n",
    "        vocab2indx[char] = len(vocab2indx)\n",
    "        \n",
    "indx2vocab = {}\n",
    "for k, v in vocab2indx.items():\n",
    "    indx2vocab[v] = k\n",
    "print(\"Vocab Size: \", len(vocab2indx))\n",
    "print(\"Total Characters:\", len(shakespear_100k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoRegressiveDataset(Dataset):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab2indx, large_string, MAX_CHUNK=500):\n",
    "        self.vocab2indx = vocab2indx\n",
    "        self.doc = large_string\n",
    "        self.MAX_CHUNK = MAX_CHUNK\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.doc)-1) // self.MAX_CHUNK\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        start = idx*self.MAX_CHUNK\n",
    "        \n",
    "        #First we build our input values x, which is just each character that \n",
    "        #occured in order. We use our vocabular to map the characters to unique\n",
    "        #IDs for the embedding layer our model will use\n",
    "        sub_string = self.doc[start:start+self.MAX_CHUNK]\n",
    "        \n",
    "        x = [self.vocab2indx[c] for c in sub_string]\n",
    "        \n",
    "        #Now we build the target values, in the exact same way as we built our\n",
    "        #inputs. The differences is that we shift the sub-string over by 1! \n",
    "        #This is because we are predicting the _next_ character! \n",
    "        sub_string = self.doc[start+1:start+self.MAX_CHUNK+1]\n",
    "        \n",
    "        y = [self.vocab2indx[c] for c in sub_string]\n",
    "        \n",
    "        return torch.tensor(x, dtype=torch.int64), torch.tensor(y, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoRegressive(nn.Module):\n",
    "\n",
    "    def __init__(self, num_embeddings, embd_size, hidden_size, padding_idx=None, layers=1):\n",
    "        \"\"\"\n",
    "        num_embeddings: this is the size of our vocabulary, as each item will require its own embedding. \n",
    "        embd_size: how many dimensions to use for the embedding layer\n",
    "        hidden_size: how many units to use in the RNN layers\n",
    "        padding_idx: the token used to indicate padding\n",
    "        layers:  the number of GRU layers to use in the auto-encoder. \n",
    "        \"\"\"\n",
    "        super(AutoRegressive, self).__init__()\n",
    "        self.padding_idx = padding_idx\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embd = nn.Embedding(num_embeddings, embd_size, padding_idx=padding_idx)\n",
    "        self.layers = nn.ModuleList([nn.GRUCell(embd_size, hidden_size)] + \n",
    "                                      [nn.GRUCell(hidden_size, hidden_size) for i in range(layers-1)])\n",
    "        self.norms = nn.ModuleList([nn.LayerNorm(hidden_size) for i in range(layers)])\n",
    "        \n",
    "        self.linear1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.layernorm = nn.LayerNorm(hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, num_embeddings)\n",
    "        \n",
    "    \n",
    "    def forward(self, input):\n",
    "        #Input should be (B, T)\n",
    "        #What is the batch size?\n",
    "        B = input.size(0)\n",
    "        #What is the max number of time steps?\n",
    "        T = input.size(1)\n",
    "        \n",
    "        x = self.embd(input) #(B, T, D)\n",
    "        \n",
    "        #grab the device that the model currently resides on\n",
    "        device = self.embd.weight.device\n",
    "        \n",
    "        if self.padding_idx is not None:\n",
    "            mask = input != self.padding_idx\n",
    "        else:\n",
    "            mask = input == input\n",
    "        mask = mask.to(device)\n",
    "        #Mask is now (B, T)\n",
    "        \n",
    "        #Initial hidden states\n",
    "        h_prevs = [torch.zeros(B, self.hidden_size, device=device) for _ in range(len(self.layers))]\n",
    "        \n",
    "        last_activations = []\n",
    "        for t in range(T):\n",
    "            #Grab the mask that tells us which slices in time are currently valid\n",
    "            #we do the unsqueeze to make it shape (B, 1), so that we can do valid\n",
    "            #multiplications with the hidden states\n",
    "            mask_t = mask[:,t].unsqueeze(1) \n",
    "            \n",
    "            x_in = x[:,t,:] #(B, D)\n",
    "            \n",
    "            for l in range(len(self.layers)):\n",
    "                h_prev = h_prevs[l]\n",
    "                h = self.norms[l](self.layers[l](x_in, h_prev))\n",
    "                \n",
    "                h_prevs[l]  = h*mask_t + (~mask_t)*h_prev\n",
    "                x_in = h\n",
    "            last_activations.append(x_in)\n",
    "        \n",
    "        last_activations = torch.stack(last_activations, dim=1) #(B, T, D)\n",
    "        \n",
    "        #Apply linear models to results over time \n",
    "        h = self.layernorm(F.leaky_relu(self.linear1(last_activations)))\n",
    "        h = self.linear2(h) #(B, T, D) -> B(B, T, VocabSize)\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoRegData = AutoRegressiveDataset(vocab2indx, shakespear_100k, MAX_CHUNK=250)\n",
    "autoReg_loader = DataLoader(autoRegData, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoReg_model = AutoRegressive(len(vocab2indx), 32, 128, layers=2)\n",
    "autoReg_model = autoReg_model.to(device)\n",
    "\n",
    "for p in autoReg_model.parameters():\n",
    "    p.register_hook(lambda grad: torch.clamp(grad, -2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrossEntLossTime(x, y):\n",
    "    \"\"\"\n",
    "    x: output predictions with shape (B, T, V)\n",
    "    y: target labels with shape (B, T)\n",
    "    \n",
    "    \"\"\"\n",
    "    cel = nn.CrossEntropyLoss()\n",
    "    \n",
    "    T = x.size(1)\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    for t in range(T):\n",
    "        loss += cel(x[:,t,:], y[:,t])\n",
    "    \n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50ef7b69d9974470b0dfab7c53f16ff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=20, style=ProgressStyle(description_width='initia…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ea6a40cc36e4d77ac16c3ef0c49c1e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Train Batch', max=94, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-0cbf3ba4b4d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mautoReg_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCrossEntLossTime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautoReg_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_filename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'autoreg_model.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-cf0ca0ec5993>\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(model, loss_func, train_loader, val_loader, score_funcs, epochs, device, checkpoint_file, lr_schedule, optimizer, disable_tqdm, should_stop_early, override_early_stop_on_start)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# ∇_Θ just got computed by this one call!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0;31m#Now we just need to update all the parameters!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_network(autoReg_model, CrossEntLossTime, autoReg_loader, epochs=20, device=device, checkpoint_file=get_filename('autoreg_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
